{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import hashlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_index</th>\n",
       "      <th>text_length</th>\n",
       "      <th>capital_letter</th>\n",
       "      <th>parenthesis_flag</th>\n",
       "      <th>label</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1$1/4</td>\n",
       "      <td>I1</td>\n",
       "      <td>L20</td>\n",
       "      <td>NoCAP</td>\n",
       "      <td>NoPAREN</td>\n",
       "      <td>B-QTY</td>\n",
       "      <td>356a192b7913b04c54574d18c28d46e6395428ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cups</td>\n",
       "      <td>I2</td>\n",
       "      <td>L20</td>\n",
       "      <td>NoCAP</td>\n",
       "      <td>NoPAREN</td>\n",
       "      <td>B-UNIT</td>\n",
       "      <td>356a192b7913b04c54574d18c28d46e6395428ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cooked</td>\n",
       "      <td>I3</td>\n",
       "      <td>L20</td>\n",
       "      <td>NoCAP</td>\n",
       "      <td>NoPAREN</td>\n",
       "      <td>B-COMMENT</td>\n",
       "      <td>356a192b7913b04c54574d18c28d46e6395428ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>I4</td>\n",
       "      <td>L20</td>\n",
       "      <td>NoCAP</td>\n",
       "      <td>NoPAREN</td>\n",
       "      <td>I-COMMENT</td>\n",
       "      <td>356a192b7913b04c54574d18c28d46e6395428ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pureed</td>\n",
       "      <td>I5</td>\n",
       "      <td>L20</td>\n",
       "      <td>NoCAP</td>\n",
       "      <td>NoPAREN</td>\n",
       "      <td>I-COMMENT</td>\n",
       "      <td>356a192b7913b04c54574d18c28d46e6395428ab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text text_index text_length capital_letter parenthesis_flag      label  \\\n",
       "0   1$1/4         I1         L20          NoCAP          NoPAREN      B-QTY   \n",
       "1    cups         I2         L20          NoCAP          NoPAREN     B-UNIT   \n",
       "2  cooked         I3         L20          NoCAP          NoPAREN  B-COMMENT   \n",
       "3     and         I4         L20          NoCAP          NoPAREN  I-COMMENT   \n",
       "4  pureed         I5         L20          NoCAP          NoPAREN  I-COMMENT   \n",
       "\n",
       "                                         ID  \n",
       "0  356a192b7913b04c54574d18c28d46e6395428ab  \n",
       "1  356a192b7913b04c54574d18c28d46e6395428ab  \n",
       "2  356a192b7913b04c54574d18c28d46e6395428ab  \n",
       "3  356a192b7913b04c54574d18c28d46e6395428ab  \n",
       "4  356a192b7913b04c54574d18c28d46e6395428ab  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/raw/ingredient_phrase_tagger_training_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061069"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_id(s):\n",
    "    return hashlib.sha1(str(s).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def add_id_to_df(input_df):\n",
    "    df['ID'] = None\n",
    "    new_ingredient_loc = df.text_index.str.match(\"^I1$\")\n",
    "    ingredient_count = new_ingredient_loc.sum()\n",
    "    df.loc[new_ingredient_loc, \"ID\"] = list(map(generate_id, range(1,ingredient_count+1)))\n",
    "    return df.fillna(method = 'ffill')\n",
    "\n",
    "def column_names():\n",
    "    print(\"Renaming columns...\")\n",
    "    return [\n",
    "             'text',\n",
    "             'text_index',\n",
    "             'text_length',\n",
    "             'capital_letter',\n",
    "             'parenthesis_flag',\n",
    "             'label'\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix training data\n",
    "\n",
    "replacement_dict = {\n",
    "            \"tsp\": \"teaspoon\",\n",
    "            \"tsp.\": \"teaspoon\",\n",
    "            \"oz\": \"ounce\",\n",
    "            \"oz.\": \"ounce\",\n",
    "            \"tbsp\": \"tablespoon\",\n",
    "            \"tbsp.\": \"tablespoon\",\n",
    "            \"lb\": \"pound\",\n",
    "            \"lb.\": \"pound\",\n",
    "            \"ml\": \"milliliter\",\n",
    "            \"ml.\": \"milliliter\",\n",
    "            \"g\" : \"grams\",\n",
    "            \"g.\" : \"grams\",\n",
    "        }\n",
    "\n",
    "def remove_urls(df):\n",
    "    \"\"\"remove urls from training data\"\"\"\n",
    "    df = df.loc[~df['text'].str.contains('http')].reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "def remove_label_prefix(df):\n",
    "    print(len(df))\n",
    "    \"\"\"Remove the prefix in labels such as 'B-'\n",
    "    and 'I-'\"\"\"\n",
    "    df.loc[:, 'label'] = df.label.replace('^(.-)', '', regex= True)\n",
    "    return df\n",
    "\n",
    "def replace_index_with_qty(df):\n",
    "    print(len(df))\n",
    "    \"\"\"Remove and replace 'INDEX' label with 'QTY'\"\"\"\n",
    "    df.loc[:, 'label'] = df.label.replace('INDEX', 'QTY')\n",
    "    return df\n",
    "\n",
    "def remove_qty_symbol(df):\n",
    "    print(len(df))\n",
    "    \"\"\"Remove the '$' symbol from ingredient quantities\"\"\"\n",
    "    df.loc[:, 'text'] = df.text.replace('\\$', ' ', regex = True)\n",
    "    return df\n",
    "\n",
    "def parenthesis_correcting(input_df):\n",
    "    \"\"\"Turns parenthesis into COMMENT\"\"\"\n",
    "    print(len(input_df))\n",
    "    df = input_df.copy()\n",
    "    \n",
    "    # remove instances where there are imbalanced parenthesis\n",
    "    open_p = df.loc[(df.text.str.match('^\\($'))].groupby(\"ID\").count()[['text']].rename(columns = {\"text\":\"(\"}).reset_index(drop = False)\n",
    "    closed_p = df.loc[(df.text.str.match('^\\)$'))].groupby(\"ID\").count()[['text']].rename(columns = {\"text\":\")\"}).reset_index(drop = False)\n",
    "    matches = open_p.merge(closed_p, on = \"ID\", how = \"outer\")\n",
    "    remove_ids = list(matches[matches[\"(\"]!=matches[\")\"]]['ID'])\n",
    "    df = df.loc[~df.ID.isin(remove_ids)]\n",
    "    \n",
    "    opened = iter(df.loc[df.text.str.match('^\\($')].index)\n",
    "    closed = iter(df.loc[df.text.str.match('^\\)$')].index)\n",
    "    id_iter = iter(df.drop_duplicates(\"ID\").index)  # indexes for when new ingredient starts\n",
    "    # intialize\n",
    "    o = next(opened)\n",
    "    c = next(closed)\n",
    "    id_ = next(id_iter)\n",
    "    arr = []\n",
    "    for i in range(len(df)):\n",
    "        while c < o:\n",
    "            c = next(closed)\n",
    "        if i == id_:\n",
    "            id_ = next(id_iter)\n",
    "            var = False\n",
    "        if var:\n",
    "            if i > c:\n",
    "                arr.append(False)\n",
    "                c = next(closed)\n",
    "            else:\n",
    "                arr.append(True)\n",
    "            continue\n",
    "        if i < o:\n",
    "            var = False\n",
    "            arr.append(var)\n",
    "            continue\n",
    "        if (i == o) | (i == c):\n",
    "            var = True\n",
    "            if i == o:\n",
    "                o = next(opened)\n",
    "        arr.append(var)\n",
    "    # make parenthesis comments\n",
    "    # if removing parenthsis, switch True and False then filter out.\n",
    "    df.loc[arr,\"label\"] = \"COMMENT\"\n",
    "    return df\n",
    "\n",
    "def remove_hyphen_ingredients(df):\n",
    "    \"\"\"Find and remove units and qtys with hyphens in them.\n",
    "    These instances happen in examples like '1-pound'\n",
    "    and is incorrectly labelled just a qty or just a unit.\n",
    "    \"\"\"\n",
    "    print(len(df))\n",
    "    d = df.copy()\n",
    "    hyphen_in = d.text.str.contains(\"-\")\n",
    "    not_hyphen_only = ~d.text.str.match('^-$')\n",
    "    qty = d.label.str.match('^QTY$') \n",
    "    unit = d.label.str.match('^UNIT$')\n",
    "    qty_or_unit = (qty | unit)\n",
    "    filter_ids = d.loc[(not_hyphen_only & (hyphen_in & qty_or_unit))].ID\n",
    "    return d[~d['ID'].isin(filter_ids)].reset_index(drop=True)\n",
    "\n",
    "def or_to_comment(df):\n",
    "    \"\"\"\n",
    "    Check that the first NAME comes before the first \"or\".\n",
    "    If True, it typically means the text following the or\n",
    "    is an alternative to the first NAME. If this is False,\n",
    "    typically it means there are two comments to the NAME\n",
    "    e.g. chicken or beef stock - where \"stock\" is the NAME\n",
    "    and \"chicken or beef\" is the COMMENT. This scenario is\n",
    "    fine for the parser, however it may be worth it to\n",
    "    `split` this after the parser is applied.\n",
    "    \n",
    "    If there are two or more 'or's, we will see which ones\n",
    "    come after the first or and replace all text after them\n",
    "    as a comment.\n",
    "    \"\"\"\n",
    "    print(len(df))\n",
    "    or_ingr = df.copy()\n",
    "    first_name_ingredients = or_ingr[or_ingr['label'] ==  \"NAME\"].drop_duplicates(\"ID\")\n",
    "    ingredient_with_name_id = list(first_name_ingredients.ID)\n",
    "    or_ingr = or_ingr[or_ingr['ID'].isin(ingredient_with_name_id)].reset_index(drop = True) # filter out ingredients with no names in them.\n",
    "    print(len(or_ingr))\n",
    "    or_index = or_ingr[or_ingr['text'] ==  \"or\"].index # get index of 'or'\n",
    "    first_name_index = first_name_ingredients.index # index of first names for each ingredient\n",
    "    new_id_index = or_ingr.drop_duplicates(\"ID\").index  # indexes for when new ingredient starts\n",
    "\n",
    "    iter_id = iter(new_id_index)\n",
    "    iter_name = iter(first_name_index)\n",
    "    iter_or = iter(or_index)\n",
    "    id_ = next(iter_id) # intialize the ID\n",
    "    or_ = next(iter_or) # initialize the or index\n",
    "    \n",
    "    arr = []\n",
    "    for i in range(len(or_ingr)):\n",
    "        if i == id_: # new ingredient, reset val\n",
    "            val = False\n",
    "            id_ = next(iter_id, None) # get next ingredient index\n",
    "            name_ = next(iter_name, None) # initialize the name\n",
    "\n",
    "        if val: # if we are commenting out, continue to comment out\n",
    "            if i == or_:\n",
    "                or_ = next(iter_or, None) # if more 'or's appear in alternative ingredients, skip them.\n",
    "            arr.append(True)\n",
    "            continue\n",
    "\n",
    "        if i == or_: # if the text is or, evaluate\n",
    "\n",
    "            if name_ < or_: # if name came before or, comment out the or's\n",
    "                val = True # initialize the comment out markers\n",
    "            else:\n",
    "                val = False\n",
    "            or_ = next(iter_or, None)\n",
    "\n",
    "        arr.append(val)\n",
    "\n",
    "    or_ingr.loc[arr, \"label\"] = \"COMMENT\"\n",
    "    return or_ingr\n",
    "\n",
    "def bad_qty_unit_entries(input_df):\n",
    "    df = input_df.copy()\n",
    "    print(len(df))\n",
    "    bad_qty_and_unit = df.ID.isin(df[(df.text.str.contains(\"\\d\", regex = True))\n",
    "                                     & (df.text.str.contains(\"-\"))\n",
    "                                     & (df.label == \"OTHER\")]['ID'].unique())\n",
    "    return df.loc[~bad_qty_and_unit]\n",
    "    \n",
    "def hyphen_replacement(input_df):\n",
    "    df = input_df.copy()\n",
    "    print(len(df))\n",
    "    df.loc[:, 'text'] = df.text.replace('-', ' ', regex = True)\n",
    "    return df\n",
    "\n",
    "# Write out some bad data to be labelled later\n",
    "def explode(input_df, lst_col=\"text\", fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    df = input_df.copy()\n",
    "    print(len(df))\n",
    "    df.loc[:, lst_col] = df[lst_col].str.split(\" \")\n",
    "    if (lst_col is not None\n",
    "        and len(lst_col) > 0\n",
    "        and not isinstance(lst_col, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_col = [lst_col]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_col)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_col[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_col}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res\n",
    "\n",
    "def fix_other_labels(input_df):\n",
    "    \"\"\"Most times that a period appeared it was labelled as OTHER.\n",
    "    Replace those instances with a COMMENT label. Ensure that when\n",
    "    the text is just a period to label as OTHER\"\"\"\n",
    "    df = input_df.copy()\n",
    "    print(len(df))\n",
    "    period = df.text.str.contains(\"\\.\")\n",
    "    other = df.label.str.match(\"OTHER\")\n",
    "    just_period = df.text.str.contains(\"^\\.$\")\n",
    "    df.loc[((period) & (other) & (~just_period)), \"label\"] = \"COMMENT\"\n",
    "    df.loc[just_period, \"label\"] = \"OTHER\"\n",
    "    return df\n",
    "\n",
    "def fix_commented_units(input_df):\n",
    "    df = input_df.copy()\n",
    "    regex_string = \"$|^\".join(replacement_dict.keys())\n",
    "    regex_string = \"^\" + regex_string + \"$\"\n",
    "    regex_string = regex_string.replace(\".\", \"\\.\")\n",
    "    df.loc[df.text.str.lower().str.contains(regex_string, regex=True), \"label\"] = \"UNIT\"\n",
    "    return df\n",
    "\n",
    "def replace_short_units(input_df):\n",
    "    df = input_df.copy()\n",
    "    def replacement_function(text):\n",
    "        lowercase_text = text.lower() \n",
    "        if lowercase_text in replacement_dict:\n",
    "              return replacement_dict[lowercase_text]\n",
    "        else:\n",
    "            return text\n",
    "    df.loc[:, \"text\"] = df.text.apply(replacement_function)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def run_data_cleaning(df, *funcs):\n",
    "    \"\"\"\n",
    "    Cleanses training data of text that increases complexity\n",
    "    or that will cause issues while creating the model.\n",
    "    \"\"\"\n",
    "    print(\"Cleaning phrase tagger data...\")\n",
    "    return reduce(lambda arg, func: func(arg), funcs, df)[['text', 'label', 'ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning phrase tagger data...\n",
      "1060161\n",
      "1060161\n",
      "1060161\n",
      "1060161\n",
      "1058388\n",
      "1047902\n",
      "1047902\n",
      "1076718\n",
      "1076718\n",
      "1076109\n",
      "1065872\n"
     ]
    }
   ],
   "source": [
    "df = run_data_cleaning(\n",
    "    df, \n",
    "    remove_urls,\n",
    "    remove_label_prefix,\n",
    "    replace_index_with_qty,\n",
    "    remove_qty_symbol,\n",
    "    remove_hyphen_ingredients,\n",
    "    bad_qty_unit_entries,\n",
    "    hyphen_replacement,\n",
    "    explode,\n",
    "    fix_other_labels,\n",
    "    fix_commented_units,\n",
    "    replace_short_units,\n",
    "    parenthesis_correcting,\n",
    "    or_to_comment,\n",
    "\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fd40a4946525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mbad_qty_and_unit_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbad_qty_and_unit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#bad_qty_and_unit_df.loc[:, \"text\"] = bad_qty_and_unit_df.text.str.split(\"-\") # make text list-like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mbad_qty_and_unit_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_qty_and_unit_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m#bad_qty_and_unit_df[[\"ID\", \"text\", \"label\"]].to_csv(\"../data/interim/NEEDS_LABELLED_bad_qty_unit_ingredient_phrase_tagger_training_data.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-fd40a4946525>\u001b[0m in \u001b[0;36mexplode\u001b[0;34m(input_df, lst_col, fill_value, preserve_index)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 index=idx)\n\u001b[1;32m     21\u001b[0m              .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n\u001b[0;32m---> 22\u001b[0;31m                             for col in lst_cols}))\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# append those rows that have empty lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-fd40a4946525>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 index=idx)\n\u001b[1;32m     21\u001b[0m              .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n\u001b[0;32m---> 22\u001b[0;31m                             for col in lst_cols}))\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# append those rows that have empty lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# only works if data cleaning above doesn't run `bad_qty_unit_entries`\n",
    "\n",
    "#Write out some bad data to be labelled later\n",
    "def explode(input_df, lst_col=\"text\", fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    df = input_df.copy()\n",
    "    df.loc[:, lst_col] = df[lst_col].str.split(\"-\")\n",
    "    if (lst_col is not None\n",
    "        and len(lst_col) > 0\n",
    "        and not isinstance(lst_col, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_col]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res\n",
    "\n",
    "bad_qty_and_unit = df.ID.isin(df[(df.text.str.contains(\"\\d\", regex = True)) & (df.text.str.contains(\"-\")) & (df.label == \"OTHER\")]['ID'].unique())\n",
    "bad_qty_and_unit_df = df.loc[bad_qty_and_unit]\n",
    "#bad_qty_and_unit_df.loc[:, \"text\"] = bad_qty_and_unit_df.text.str.split(\"-\") # make text list-like\n",
    "bad_qty_and_unit_df = explode(bad_qty_and_unit_df)\n",
    "#bad_qty_and_unit_df[[\"ID\", \"text\", \"label\"]].to_csv(\"../data/interim/NEEDS_LABELLED_bad_qty_unit_ingredient_phrase_tagger_training_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
